![coval](coval.png)

# Intelligent Code Generation, Execution, and Repair ü§ñ COVAL Developer Assistant  

![COVAL](https://img.shields.io/badge/COVAL-v2.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.11+-green.svg)
![Docker](https://img.shields.io/badge/Docker-Required-blue.svg)
![Ollama](https://img.shields.io/badge/Ollama-Required-orange.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)

**COVAL** is a comprehensive Python package that manages iterative code generation, execution, and repair with multiple LLM models, integrated Docker Compose deployments, transparent volume overlays, legacy cleanup, and adaptive cost optimization to enable efficient and scalable automated code repair workflows.


```
                    START
                      ‚îÇ
                  [TRIAGE]
                      ‚îÇ
              Collect Metrics
                      ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  DECISION MODEL   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
           C_fix > 1.5 * C_new?
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                YES        NO
                 ‚îÇ          ‚îÇ
            [REBUILD]   [REPAIR]
                 ‚îÇ          ‚îÇ
           Recommend      MRE
           Rebuilding   Creation
                 ‚îÇ          ‚îÇ
                END    Fix Generation
                           ‚îÇ
                      Validation
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   PASS        FAIL
                     ‚îÇ          ‚îÇ
                 [SUCCESS]  Retry?
                     ‚îÇ      ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê
                    END    YES   NO
                            ‚îÇ     ‚îÇ
                      Next Iter  END
```



## üöÄ Quick Start

```bash
# Install COVAL
pip install -e .

# Generate new code
coval generate -d "Create a FastAPI app with user authentication" --deploy

# Check status
coval status

# Repair issues
coval repair -e error.log --deploy

# Cleanup old iterations
coval cleanup -c 5
```

## ‚ú® Key Features

### üîÑ **Iterative Code Management**
- **Intelligent Iteration System**: Each generation/repair creates a new versioned iteration
- **Cost-Based Decisions**: Automatic analysis of whether to modify existing code or generate new
- **Legacy Cleanup**: Automatic removal of old iterations with configurable retention policies
- **History Tracking**: Complete audit trail of all code changes and decisions


```
1. INPUT
   ‚îú‚îÄ‚îÄ Error File (stacktrace/logs)
   ‚îú‚îÄ‚îÄ Source Directory
   ‚îî‚îÄ‚îÄ Test File (optional)
          ‚Üì
2. TRIAGE
   ‚îú‚îÄ‚îÄ Calculate Technical Debt
   ‚îú‚îÄ‚îÄ Measure Test Coverage
   ‚îú‚îÄ‚îÄ Assess Available Context
   ‚îî‚îÄ‚îÄ Get Model Capability
          ‚Üì
3. DECISION
   ‚îú‚îÄ‚îÄ Calculate Repair Cost
   ‚îú‚îÄ‚îÄ Calculate Rebuild Cost
   ‚îî‚îÄ‚îÄ Make Decision (repair/rebuild)
          ‚Üì
4. MRE CREATION [if repair]
   ‚îú‚îÄ‚îÄ Copy Relevant Files
   ‚îú‚îÄ‚îÄ Create Dockerfile
   ‚îî‚îÄ‚îÄ Generate README
          ‚Üì
5. FIX GENERATION
   ‚îú‚îÄ‚îÄ Prepare Context
   ‚îú‚îÄ‚îÄ Generate Prompt
   ‚îú‚îÄ‚îÄ Call LLM
   ‚îî‚îÄ‚îÄ Parse Response
          ‚Üì
6. VALIDATION
   ‚îú‚îÄ‚îÄ Apply Patch
   ‚îú‚îÄ‚îÄ Build Container
   ‚îú‚îÄ‚îÄ Run Tests
   ‚îî‚îÄ‚îÄ Check Results
          ‚Üì
7. INTEGRATION
   ‚îú‚îÄ‚îÄ Save Final Patch
   ‚îú‚îÄ‚îÄ Generate Report
   ‚îî‚îÄ‚îÄ Return Result
```

### ü§ñ **Multi-LLM Code Generation & Repair** 
- **6 Specialized Models**: Qwen, DeepSeek-R1, CodeLlama 13B, DeepSeek, Granite, Mistral
- **Adaptive Model Selection**: Choose optimal model based on task complexity and context
- **Automatic Model Management**: Download and configure models automatically via Ollama
- **Dynamic Capability Calculation**: Real-time model performance assessment

### üê≥ **Transparent Docker Deployments**
- **Blue-Green Deployments**: Zero-downtime deployments with automatic rollback
- **Volume Overlays**: Expose only latest changes while preserving legacy code
- **Multi-Framework Support**: FastAPI, Flask, Express.js, Next.js templates
- **Health Monitoring**: Automatic health checks and failure detection

### üí° **Intelligent Cost Analysis**
- **Cost Calculator**: Automatically decides between modifying existing code vs generating new
- **Multi-Factor Analysis**: Considers technical debt, scope, complexity, and historical success
- **Risk Assessment**: Evaluates confidence levels and potential regression risks
- **Optimization Suggestions**: Recommends best approach for each scenario



## üìã CLI Commands

### `coval generate` - Generate New Code
```bash
# Basic generation
coval generate -d "Create a REST API for user management" --model deepseek-r1

# Specify framework and features
coval generate -d "Build a blog platform" -f fastapi -l python \
  --features "authentication" --features "database" --deploy

# Generate from parent iteration
coval generate -d "Add payment system" --parent iter-001 --model deepseek-r1
```

### `coval run` - Deploy Iterations
```bash
# Deploy latest iteration
coval run

# Deploy specific iteration
coval run -i iter-003 -p 8080

# Use different deployment strategy
coval run -i iter-002 --strategy copy
```

### `coval repair` - Fix Code Issues
```bash
# Basic repair
coval repair -e logs/error.log

# Advanced repair with specific model
coval repair -e error.log -i iter-002 --model codellama13b --deploy

# Analyze only (no repair)
coval repair -e error.log --analyze
```

### `coval status` - Project Overview
```bash
# Show all iterations and deployments
coval status

# Verbose output
coval status -v
```

### `coval cleanup` - Maintenance
```bash
# Keep only 10 most recent iterations
coval cleanup -c 10

# Force cleanup without confirmation
coval cleanup -c 5 --force
```

### `coval stop` - Stop Deployments
```bash
# Stop specific deployment
coval stop -i iter-003

# Stop all deployments
coval stop
```

## üõ† Installation


## üß∞ Makefile Automation

The repository includes a comprehensive `Makefile` that streamlines the full development and release workflow. Use the commands below to get productive quickly:

### Environment & Dependencies

```bash
make setup          # Create virtualenv and install dev dependencies
make install        # Install runtime dependencies only
make install-docs   # Install documentation toolchain
```

### Code Quality & Testing

- **`make format`** ‚Äì Format the codebase with Black.
- **`make lint`** ‚Äì Run Black, Flake8, and MyPy checks.
- **`make test`** ‚Äì Execute the full pytest suite with coverage.
- **`make quick-test`** ‚Äì Fast iteration loop (`format` + `test-fast`).
- **`make full-check`** ‚Äì Complete verification (`format`, `lint`, `test`, `security-check`).

### Build & Deployment

- **`make build`** ‚Äì Produce source and wheel distributions.
- **`make docker-build`** ‚Äì Build the Docker image tagged with the current version and `latest`.
- **`make deploy-local`** ‚Äì Build artifacts and run the Docker container locally.

### Release Automation

- **`make publish`** ‚Äì Automatically bumps the patch version, builds the project, and uploads it to PyPI.
- **`make publish-test`** ‚Äì Publish artifacts to TestPyPI.
- **`make publish-docker`** ‚Äì Push Docker images to the configured registry.
- **`make release-patch`**, **`make release-minor`**, **`make release-major`** ‚Äì Run quality gates, bump versions, build artifacts, and publish to PyPI & Docker.

> **Note:** The `make publish` target automatically increments the patch version via `make version-patch` before uploading. This prevents accidental attempts to reuse an existing version on PyPI.

### Version Management Workflow

```bash
make version          # Display current version and active git branch
make version-patch    # Bump X.Y.Z ‚Üí X.Y.(Z+1), commit, and tag
make version-minor    # Bump X.Y.Z ‚Üí X.(Y+1).0, commit, and tag
make version-major    # Bump X.Y.Z ‚Üí (X+1).0.0, commit, and tag
```

Each command updates `setup.py` and `coval/__init__.py`, creates a git commit, and produces an annotated tag (e.g., `v2.0.1`).

For full releases:

```bash
make release-patch    # format/lint/test ‚Üí version-patch ‚Üí build ‚Üí publish ‚Üí docker-push
make release-minor
make release-major
```

If a publication fails after a version bump, you can roll back by deleting the tag and resetting the commit:

```bash
git tag -d v<new_version>
git reset --hard HEAD^   # restore previous commit
```


### Prerequisites
```bash
# System requirements
Python 3.11+
Docker & Docker Compose
Ollama (for LLM models)

# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

### Install COVAL
```bash
# Clone repository
git clone https://github.com/your-org/coval.git
cd coval

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install package
pip install -e .

# Verify installation
coval --help
```

Or use the Makefile helper (recommended):

```bash
make setup            # Creates venv, installs runtime + dev dependencies
source venv/bin/activate
coval --help
```

## ‚öôÔ∏è Configuration

### Project Configuration (`coval.config.yaml`)
```yaml
# Project settings
project:
  name: "my-coval-project"
  framework: "auto-detect"
  language: "auto-detect" 
  max_iterations: 50

# Docker deployment settings  
docker:
  base_port: 8000
  network_name: "coval-network"
  auto_cleanup: true

# Volume overlay strategy
volumes:
  strategy: "overlay"  # overlay, copy, symlink
  preserve_permissions: true

# Cost calculation settings
cost_calculation:
  modify_base_cost: 10.0
  generate_base_cost: 25.0
  complexity_multiplier: 2.0
```

### LLM Configuration (`llm.config.yaml`)
```yaml
models:
  qwen2.5-coder:
    model_name: "qwen2.5-coder:7b"
    max_tokens: 16384
    temperature: 0.2
    base_capability: 0.85
    context_window: 32768
    
  deepseek-r1:
    model_name: "deepseek-r1:7b" 
    max_tokens: 12288
    temperature: 0.1
    base_capability: 0.80
    context_window: 16384
```

### üîÑ **Automatyczne Pobieranie Modeli**
System automatycznie sprawdza dostƒôpno≈õƒá modelu via `ollama list` i pobiera brakujƒÖce:
```bash
üîç Sprawdzam dostƒôpno≈õƒá modelu: deepseek-r1:7b
üì• Pobieram model: deepseek-r1:7b
‚úÖ Pomy≈õlnie pobrano model: deepseek-r1:7b
```

## Kluczowe funkcjonalno≈õci:

### 1. **Model Decyzyjny Repair vs Rebuild**
- Implementuje matematyczny model kosztu naprawy: `C_fix = Œ≥D * (1/S) * (1/K) * (1 + Œª(1-T))`
- Oblicza prawdopodobie≈Ñstwo sukcesu u≈ºywajƒÖc **funkcji logitowej**
- Automatycznie decyduje czy naprawiaƒá czy przebudowaƒá

### 2. **Workflow Naprawy (MRE ‚Üí Test ‚Üí Patch ‚Üí Walidacja)**
- **Triage**: Analiza problemu i zbieranie metryk
- **MRE**: Tworzenie Minimal Reproducible Example
- **Generowanie**: U≈ºywa LLM do tworzenia poprawek
- **Walidacja**: Automatyczne testy w Docker
- **Integracja**: Finalizacja i raportowanie

### 3. **Metryki i Analiza**
- D≈Çug techniczny (z≈Ço≈ºono≈õƒá, duplikacja, brak dokumentacji)
- Pokrycie testami
- Dostƒôpny kontekst (stacktrace, testy, dokumentacja)
- Zdolno≈õci modelu LLM

### 4. **Struktura Folder√≥w**
```
/repairs/
  /repair-{ticket-id}/
    /mre/           # Minimal Reproducible Example
    /proposals/     # Propozycje napraw
    /validation/    # Wyniki walidacji
    decision.md     # Decyzja repair vs rebuild
    repair_report.md # Raport ko≈Ñcowy
```

### 5. **U≈ºycie CLI v2.0**

#### **Dostƒôpne Modele:**
```bash
--model qwen         # qwen2.5-coder:7b (domy≈õlny, 95% zdolno≈õci)
--model deepseek     # deepseek-coder:6.7b (80% zdolno≈õci)
--model codellama13b # codellama:13b (75% zdolno≈õci, du≈ºy kontekst)
--model deepseek-r1  # deepseek-r1:7b (88% zdolno≈õci, reasoning)
--model granite      # granite-code:8b (70% zdolno≈õci, enterprise)
--model mistral      # mistral:7b (60% zdolno≈õci, fallback)
```



### 6. **Inteligentne Funkcje v2.0**

- **Automatyczne pobieranie modeli** - System sprawdza `ollama list` i pobiera brakujƒÖce modele
- **Dynamiczne obliczanie zdolno≈õci** - Uwzglƒôdnia tokeny, temperaturƒô, kontekst i historiƒô
- **Adaptacyjne uczenie siƒô** - 8 kategorii problem√≥w z historical tracking
- **Automatyczne wykrywanie jƒôzyka/frameworka** - dostosowuje Dockerfile i proces walidacji
- **Iteracyjne poprawki** - do 5 pr√≥b z r√≥≈ºnymi podej≈õciami i konfiguracjƒÖ z YAML
- **Parsowanie b≈Çƒôd√≥w** - wyciƒÖga pliki wymienione w stacktrace
- **Generowanie prompt√≥w** - r√≥≈ºne szablony dla pierwszej i kolejnych pr√≥b
- **Walidacja w kontenerach** - izolowane ≈õrodowisko testowe
- **Konfigurowalne parametry** - Wszystkie ustawienia modeli w `llm.config.yaml`


## üîß **Troubleshooting**

### **Problemy z Ollama:**
```bash
# Ollama nie jest zainstalowane
‚ùå Ollama nie jest zainstalowane lub nie jest w PATH
‚úÖ RozwiƒÖzanie: curl -fsSL https://ollama.com/install.sh | sh

# Model nie mo≈ºe byƒá pobrany
‚ùå B≈ÇƒÖd pobierania modelu: connection refused
‚úÖ RozwiƒÖzanie: Uruchom ollama serve w osobnym terminalu

# Timeout pobierania
‚è±Ô∏è Timeout przy pobieraniu modelu: deepseek-r1:7b
‚úÖ RozwiƒÖzanie: Zwiƒôksz timeout lub pobierz rƒôcznie: ollama pull deepseek-r1:7b
```

### **Problemy z KonfiguracjƒÖ:**
```bash
# Brak llm.config.yaml
‚ùå Nie mo≈ºna za≈Çadowaƒá konfiguracji
‚úÖ RozwiƒÖzanie: Skopiuj llm.config.yaml z repozytorium

# Nieprawid≈Çowa konfiguracja YAML
‚ùå yaml.parser.ParserError
‚úÖ RozwiƒÖzanie: Sprawd≈∫ sk≈Çadniƒô YAML online (yamllint.com)

# Brak uprawnie≈Ñ do zapisu repair_history.json
‚ùå Permission denied: repairs/repair_history.json
‚úÖ RozwiƒÖzanie: mkdir -p repairs && chmod 755 repairs
```

### **Problemy z Modelami:**
```bash
# Model nie odpowiada
‚ùå Model timeout po 60s
‚úÖ RozwiƒÖzanie: U≈ºyj mniejszego modelu (--model mistral) lub zwiƒôksz timeout

# NiewystarczajƒÖca pamiƒôƒá
‚ùå CUDA out of memory
‚úÖ RozwiƒÖzanie: U≈ºyj CPU: CUDA_VISIBLE_DEVICES="" python3 repair.py

# Model daje z≈Çe wyniki
‚ùå Repair failed repeatedly
‚úÖ RozwiƒÖzanie: Spr√≥buj innego modelu z wiƒôkszymi zdolno≈õciami (--model deepseek-r1)
```

### **Customizacja dla W≈Çasnych Potrzeb:**

#### **Zwiƒôksz Zdolno≈õci Modelu:**
```yaml
models:
  custom-qwen:
    base_capability: 0.90      # ‚Üë Wy≈ºsza baza
    max_tokens: 32768          # ‚Üë Wiƒôcej token√≥w = bonus
    temperature: 0.1           # ‚Üì Ni≈ºsza temperatura = mniej penalty
    context_window: 65536      # ‚Üë Wiƒôkszy kontekst = bonus
```

#### **Dostosuj dla ≈örodowiska Produkcyjnego:**
```yaml
global:
  timeout: 120                 # ‚Üë Wiƒôcej czasu dla z≈Ço≈ºonych napraw
  max_iterations: 10           # ‚Üë Wiƒôcej pr√≥b
  adaptive_evaluation:
    history_weight: 0.5        # ‚Üë Wiƒôksza waga dla historii
    min_samples: 10            # ‚Üë Wiƒôcej danych do oceny
```

#### **Optymalizacja dla Szybko≈õci:**
```yaml
models:
  fast-mistral:
    max_tokens: 4096           # ‚Üì Mniej token√≥w = szybciej
    temperature: 0.4           # ‚Üë Wy≈ºsza = mniej precyzyjne ale szybsze
    retry_attempts: 1          # ‚Üì Mniej pr√≥b
```

### 7. **Raporty i Decyzje**

System generuje szczeg√≥≈Çowe raporty zawierajƒÖce:
- Analizƒô koszt√≥w (repair vs rebuild)
- Prawdopodobie≈Ñstwo sukcesu
- Zastosowane poprawki
- Ocenƒô ryzyka regresji
- Rekomendacje dalszych krok√≥w

### 8. **Wsparcie dla wielu jƒôzyk√≥w**

Automatycznie rozpoznaje i obs≈Çuguje:
- Python (FastAPI, Django, Flask)
- JavaScript/Node.js (Express, Next.js)
- Go (Gin, Fiber)
- Rust, Java, Ruby, PHP

Skrypt jest w pe≈Çni zintegrowany z podej≈õciem YMLL i implementuje wszystkie najlepsze praktyki z REPAIR_GUIDELINES, zapewniajƒÖc efektywny i powtarzalny proces naprawiania kodu z pomocƒÖ LLM.

## Funkcja logitowa

Funkcja **logitowa** to po prostu funkcja matematyczna u≈ºywana g≈Ç√≥wnie w statystyce i uczeniu maszynowym do przekszta≈Çcania prawdopodobie≈Ñstw w tzw. log-odds. Jest odwrotno≈õciƒÖ funkcji sigmoidalnej (logistycznej).

Dok≈Çadniej:

### Definicja

Je≈ºeli $p$ to prawdopodobie≈Ñstwo zdarzenia (0 < p < 1), funkcja logitowa jest zdefiniowana jako:

$$
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right)
$$

* $p/(1-p)$ to **odds** (szansa, ≈ºe zdarzenie nastƒÖpi vs ≈ºe nie nastƒÖpi)
* $\ln$ to logarytm naturalny

### Przyk≈Çad

* Je≈õli $p = 0.8$ (80% prawdopodobie≈Ñstwa),

$$
\text{logit}(0.8) = \ln\left(\frac{0.8}{0.2}\right) = \ln(4) \approx 1.386
$$

* Je≈õli $p = 0.5$, $\text{logit}(0.5) = \ln(1) = 0$

### Zastosowanie

* W **regresji logistycznej** logit przekszta≈Çca prawdopodobie≈Ñstwa w warto≈õƒá na osi liczbowej od $-\infty$ do $+\infty$, co pozwala modelowi liniowemu prognozowaƒá log-odds, a nastƒôpnie ≈Çatwo przekszta≈Çcaƒá z powrotem w prawdopodobie≈Ñstwo.
* W twoim kontek≈õcie (system naprawy kodu) funkcja logitowa mo≈ºe s≈Çu≈ºyƒá do obliczenia **prawdopodobie≈Ñstwa sukcesu naprawy** na podstawie r√≥≈ºnych zmiennych (jak z≈Ço≈ºono≈õƒá, dostƒôpno≈õƒá test√≥w itd.).



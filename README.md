![coval-logo.svg](coval-logo.svg)
# ü§ñ COVAL v2.0 - Intelligent Code Generation, Execution, and Repair System

![COVAL](https://img.shields.io/badge/COVAL-v2.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.11+-green.svg)
![Docker](https://img.shields.io/badge/Docker-Required-blue.svg)
![Ollama](https://img.shields.io/badge/Ollama-Required-orange.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)

**COVAL** is a comprehensive Python package that manages iterative code generation, execution, and repair with multiple LLM models, integrated Docker Compose deployments, transparent volume overlays, legacy cleanup, and adaptive cost optimization to enable efficient and scalable automated code repair workflows.

## üöÄ Quick Start

```bash
# Install COVAL
pip install -e .

# Generate new code
coval generate -d "Create a FastAPI app with user authentication" --deploy

# Check status
coval status

# Repair issues
coval repair -e error.log --deploy

# Cleanup old iterations
coval cleanup -c 5
```

## üß∞ Makefile Automation

The repository includes a comprehensive `Makefile` that streamlines the full development and release workflow. Use the commands below to get productive quickly:

### Environment & Dependencies

```bash
make setup          # Create virtualenv and install dev dependencies
make install        # Install runtime dependencies only
make install-docs   # Install documentation toolchain
```

### Code Quality & Testing

- **`make format`** ‚Äì Format the codebase with Black.
- **`make lint`** ‚Äì Run Black, Flake8, and MyPy checks.
- **`make test`** ‚Äì Execute the full pytest suite with coverage.
- **`make quick-test`** ‚Äì Fast iteration loop (`format` + `test-fast`).
- **`make full-check`** ‚Äì Complete verification (`format`, `lint`, `test`, `security-check`).

### Build & Deployment

- **`make build`** ‚Äì Produce source and wheel distributions.
- **`make docker-build`** ‚Äì Build the Docker image tagged with the current version and `latest`.
- **`make deploy-local`** ‚Äì Build artifacts and run the Docker container locally.

### Release Automation

- **`make publish`** ‚Äì Automatically bumps the patch version, builds the project, and uploads it to PyPI.
- **`make publish-test`** ‚Äì Publish artifacts to TestPyPI.
- **`make publish-docker`** ‚Äì Push Docker images to the configured registry.
- **`make release-patch`**, **`make release-minor`**, **`make release-major`** ‚Äì Run quality gates, bump versions, build artifacts, and publish to PyPI & Docker.

> **Note:** The `make publish` target automatically increments the patch version via `make version-patch` before uploading. This prevents accidental attempts to reuse an existing version on PyPI.

### Version Management Workflow

```bash
make version          # Display current version and active git branch
make version-patch    # Bump X.Y.Z ‚Üí X.Y.(Z+1), commit, and tag
make version-minor    # Bump X.Y.Z ‚Üí X.(Y+1).0, commit, and tag
make version-major    # Bump X.Y.Z ‚Üí (X+1).0.0, commit, and tag
```

Each command updates `setup.py` and `coval/__init__.py`, creates a git commit, and produces an annotated tag (e.g., `v2.0.1`).

For full releases:

```bash
make release-patch    # format/lint/test ‚Üí version-patch ‚Üí build ‚Üí publish ‚Üí docker-push
make release-minor
make release-major
```

If a publication fails after a version bump, you can roll back by deleting the tag and resetting the commit:

```bash
git tag -d v<new_version>
git reset --hard HEAD^   # restore previous commit
```

## ‚ú® Key Features

### üîÑ **Iterative Code Management**
- **Intelligent Iteration System**: Each generation/repair creates a new versioned iteration
- **Cost-Based Decisions**: Automatic analysis of whether to modify existing code or generate new
- **Legacy Cleanup**: Automatic removal of old iterations with configurable retention policies
- **History Tracking**: Complete audit trail of all code changes and decisions

### ü§ñ **Multi-LLM Code Generation & Repair** 
- **6 Specialized Models**: Qwen, DeepSeek-R1, CodeLlama 13B, DeepSeek, Granite, Mistral
- **Adaptive Model Selection**: Choose optimal model based on task complexity and context
- **Automatic Model Management**: Download and configure models automatically via Ollama
- **Dynamic Capability Calculation**: Real-time model performance assessment

### üê≥ **Transparent Docker Deployments**
- **Blue-Green Deployments**: Zero-downtime deployments with automatic rollback
- **Volume Overlays**: Expose only latest changes while preserving legacy code
- **Multi-Framework Support**: FastAPI, Flask, Express.js, Next.js templates
- **Health Monitoring**: Automatic health checks and failure detection

### üí° **Intelligent Cost Analysis**
- **Cost Calculator**: Automatically decides between modifying existing code vs generating new
- **Multi-Factor Analysis**: Considers technical debt, scope, complexity, and historical success
- **Risk Assessment**: Evaluates confidence levels and potential regression risks
- **Optimization Suggestions**: Recommends best approach for each scenario

## üìã CLI Commands

### `coval generate` - Generate New Code
```bash
# Basic generation
coval generate -d "Create a REST API for user management" --model deepseek-r1

# Specify framework and features
coval generate -d "Build a blog platform" -f fastapi -l python \
  --features "authentication" --features "database" --deploy

# Generate from parent iteration
coval generate -d "Add payment system" --parent iter-001 --model deepseek-r1
```

### `coval run` - Deploy Iterations
```bash
# Deploy latest iteration
coval run

# Deploy specific iteration
coval run -i iter-003 -p 8080

# Use different deployment strategy
coval run -i iter-002 --strategy copy
```

### `coval repair` - Fix Code Issues
```bash
# Basic repair
coval repair -e logs/error.log

# Advanced repair with specific model
coval repair -e error.log -i iter-002 --model codellama13b --deploy

# Analyze only (no repair)
coval repair -e error.log --analyze
```

### `coval status` - Project Overview
```bash
# Show all iterations and deployments
coval status

# Verbose output
coval status -v
```

### `coval cleanup` - Maintenance
```bash
# Keep only 10 most recent iterations
coval cleanup -c 10

# Force cleanup without confirmation
coval cleanup -c 5 --force
```

### `coval stop` - Stop Deployments
```bash
# Stop specific deployment
coval stop -i iter-003

# Stop all deployments
coval stop
```

## üõ† Installation

### Prerequisites
```bash
# System requirements
Python 3.11+
Docker & Docker Compose
Ollama (for LLM models)

# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

### Install COVAL
```bash
# Clone repository
git clone https://github.com/your-org/coval.git
cd coval

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install package
pip install -e .

# Verify installation
coval --help
```

Or use the Makefile helper (recommended):

```bash
make setup            # Creates venv, installs runtime + dev dependencies
source venv/bin/activate
coval --help
```

## ‚öôÔ∏è Configuration

### Project Configuration (`coval.config.yaml`)
```yaml
# Project settings
project:
  name: "my-coval-project"
  framework: "auto-detect"
  language: "auto-detect" 
  max_iterations: 50

# Docker deployment settings  
docker:
  base_port: 8000
  network_name: "coval-network"
  auto_cleanup: true

# Volume overlay strategy
volumes:
  strategy: "overlay"  # overlay, copy, symlink
  preserve_permissions: true

# Cost calculation settings
cost_calculation:
  modify_base_cost: 10.0
  generate_base_cost: 25.0
  complexity_multiplier: 2.0
```

### LLM Configuration (`llm.config.yaml`)
```yaml
models:
  qwen2.5-coder:
    model_name: "qwen2.5-coder:7b"
    max_tokens: 16384
    temperature: 0.2
    base_capability: 0.85
    context_window: 32768
    
  deepseek-r1:
    model_name: "deepseek-r1:7b" 
    max_tokens: 12288
    temperature: 0.1
    base_capability: 0.80
    context_window: 16384
```

### üîÑ **Automatyczne Pobieranie Modeli**
System automatycznie sprawdza dostƒôpno≈õƒá modelu via `ollama list` i pobiera brakujƒÖce:
```bash
üîç Sprawdzam dostƒôpno≈õƒá modelu: deepseek-r1:7b
üì• Pobieram model: deepseek-r1:7b
‚úÖ Pomy≈õlnie pobrano model: deepseek-r1:7b
```

## Kluczowe funkcjonalno≈õci:

### 1. **Model Decyzyjny Repair vs Rebuild**
- Implementuje matematyczny model kosztu naprawy: `C_fix = Œ≥D * (1/S) * (1/K) * (1 + Œª(1-T))`
- Oblicza prawdopodobie≈Ñstwo sukcesu u≈ºywajƒÖc **funkcji logitowej**
- Automatycznie decyduje czy naprawiaƒá czy przebudowaƒá

### 2. **Workflow Naprawy (MRE ‚Üí Test ‚Üí Patch ‚Üí Walidacja)**
- **Triage**: Analiza problemu i zbieranie metryk
- **MRE**: Tworzenie Minimal Reproducible Example
- **Generowanie**: U≈ºywa LLM do tworzenia poprawek
- **Walidacja**: Automatyczne testy w Docker
- **Integracja**: Finalizacja i raportowanie

### 3. **Metryki i Analiza**
- D≈Çug techniczny (z≈Ço≈ºono≈õƒá, duplikacja, brak dokumentacji)
- Pokrycie testami
- Dostƒôpny kontekst (stacktrace, testy, dokumentacja)
- Zdolno≈õci modelu LLM

### 4. **Struktura Folder√≥w**
```
/repairs/
  /repair-{ticket-id}/
    /mre/           # Minimal Reproducible Example
    /proposals/     # Propozycje napraw
    /validation/    # Wyniki walidacji
    decision.md     # Decyzja repair vs rebuild
    repair_report.md # Raport ko≈Ñcowy
```

### 5. **U≈ºycie CLI v2.0**

#### **Dostƒôpne Modele:**
```bash
--model qwen         # qwen2.5-coder:7b (domy≈õlny, 95% zdolno≈õci)
--model deepseek     # deepseek-coder:6.7b (80% zdolno≈õci)
--model codellama13b # codellama:13b (75% zdolno≈õci, du≈ºy kontekst)
--model deepseek-r1  # deepseek-r1:7b (88% zdolno≈õci, reasoning)
--model granite      # granite-code:8b (70% zdolno≈õci, enterprise)
--model mistral      # mistral:7b (60% zdolno≈õci, fallback)
```

#### **Przyk≈Çady U≈ºycia:**

```bash
# Podstawowa naprawa z najlepszym modelem
python3 repair.py --error error.log --source ./src

# Analiza z pokazaniem dynamicznych zdolno≈õci modelu
python3 repair.py --analyze --source ./src --error error.txt --model qwen

# U≈ºyj zaawansowanego modelu z reasoning do z≈Ço≈ºonych b≈Çƒôd√≥w
python3 repair.py --error complex_bug.log --source ./app --model deepseek-r1

# Du≈ºy kontekst dla z≈Ço≈ºonych projekt√≥w
python3 repair.py --error error.txt --source ./large_project --model codellama13b

# Z testem i verbose logging
python3 repair.py --error stacktrace.txt --source ./project \
  --test tests/test_bug.py --model qwen --verbose

# Enterprise-grade model dla produkcji
python3 repair.py --error prod_error.log --source ./enterprise_app \
  --model granite --ticket PROD-1234

# Tylko analiza z por√≥wnaniem modeli
python3 repair.py --analyze --source ./src --error bug.txt --model deepseek-r1
python3 repair.py --analyze --source ./src --error bug.txt --model codellama13b
```

#### **Przyk≈Çad Wyj≈õcia z v2.0:**
```bash
$ python3 repair.py --analyze --source ./app --error error.log --model qwen

üîç Sprawdzam dostƒôpno≈õƒá modelu: qwen2.5-coder:7b
‚úÖ Model qwen2.5-coder:7b jest dostƒôpny
ü§ñ U≈ºyto modelu: qwen2.5-coder:7b
‚öôÔ∏è  Konfiguracja: 16384 token√≥w, temp: 0.2
üìä Tryb analizy (bez naprawy)

============================================================
üìä ANALIZA DECYZYJNA v2.0
============================================================
Rekomendacja: REBUILD
Prawdopodobie≈Ñstwo sukcesu: 65.63%
Koszt naprawy: 1052.63 ‚Üì (ni≈ºszy dziƒôki lepszej zdolno≈õci!)
Koszt przebudowy: 10.18

Metryki:
  - D≈Çug techniczny: 2.00
  - Pokrycie testami: 0.00%
  - Dostƒôpny kontekst: 0.00%
  - Zdolno≈õci modelu: 95.00% ‚Üë (dynamiczne!)
  - Historyczna skuteczno≈õƒá: 0.00% (nowy system)
  - Kategoria problemu: import_error
  - Model u≈ºyty: qwen2.5-coder:7b
  - Parametry: 16384 token√≥w, temp: 0.2
============================================================
```

### 6. **Inteligentne Funkcje v2.0**

- **Automatyczne pobieranie modeli** - System sprawdza `ollama list` i pobiera brakujƒÖce modele
- **Dynamiczne obliczanie zdolno≈õci** - Uwzglƒôdnia tokeny, temperaturƒô, kontekst i historiƒô
- **Adaptacyjne uczenie siƒô** - 8 kategorii problem√≥w z historical tracking
- **Automatyczne wykrywanie jƒôzyka/frameworka** - dostosowuje Dockerfile i proces walidacji
- **Iteracyjne poprawki** - do 5 pr√≥b z r√≥≈ºnymi podej≈õciami i konfiguracjƒÖ z YAML
- **Parsowanie b≈Çƒôd√≥w** - wyciƒÖga pliki wymienione w stacktrace
- **Generowanie prompt√≥w** - r√≥≈ºne szablony dla pierwszej i kolejnych pr√≥b
- **Walidacja w kontenerach** - izolowane ≈õrodowisko testowe
- **Konfigurowalne parametry** - Wszystkie ustawienia modeli w `llm.config.yaml`

## üì¶ **Instalacja i Wymagania v2.0**

### **Wymagania Systemowe:**
```bash
# Podstawowe wymagania
Python 3.11+
Docker & Docker Compose
Ollama (automatyczne pobieranie modeli)

# Python dependencies
pip install pyyaml requests docker subprocess32
```

### **Instalacja Ollama:**
```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Pobierz z https://ollama.com/download

# Uruchom ollama
ollama serve
```

### **Konfiguracja COVAL:**
```bash
# Klonuj repozytorium
git clone https://github.com/tw√≥j-repo/ymll.git
cd ymll/coval

# System automatycznie pobierze modele przy pierwszym u≈ºyciu
python3 repair.py --analyze --source ./test --error ./test.log --model qwen

# Sprawd≈∫ dostƒôpne modele
ollama list
```

## üîß **Troubleshooting**

### **Problemy z Ollama:**
```bash
# Ollama nie jest zainstalowane
‚ùå Ollama nie jest zainstalowane lub nie jest w PATH
‚úÖ RozwiƒÖzanie: curl -fsSL https://ollama.com/install.sh | sh

# Model nie mo≈ºe byƒá pobrany
‚ùå B≈ÇƒÖd pobierania modelu: connection refused
‚úÖ RozwiƒÖzanie: Uruchom ollama serve w osobnym terminalu

# Timeout pobierania
‚è±Ô∏è Timeout przy pobieraniu modelu: deepseek-r1:7b
‚úÖ RozwiƒÖzanie: Zwiƒôksz timeout lub pobierz rƒôcznie: ollama pull deepseek-r1:7b
```

### **Problemy z KonfiguracjƒÖ:**
```bash
# Brak llm.config.yaml
‚ùå Nie mo≈ºna za≈Çadowaƒá konfiguracji
‚úÖ RozwiƒÖzanie: Skopiuj llm.config.yaml z repozytorium

# Nieprawid≈Çowa konfiguracja YAML
‚ùå yaml.parser.ParserError
‚úÖ RozwiƒÖzanie: Sprawd≈∫ sk≈Çadniƒô YAML online (yamllint.com)

# Brak uprawnie≈Ñ do zapisu repair_history.json
‚ùå Permission denied: repairs/repair_history.json
‚úÖ RozwiƒÖzanie: mkdir -p repairs && chmod 755 repairs
```

### **Problemy z Modelami:**
```bash
# Model nie odpowiada
‚ùå Model timeout po 60s
‚úÖ RozwiƒÖzanie: U≈ºyj mniejszego modelu (--model mistral) lub zwiƒôksz timeout

# NiewystarczajƒÖca pamiƒôƒá
‚ùå CUDA out of memory
‚úÖ RozwiƒÖzanie: U≈ºyj CPU: CUDA_VISIBLE_DEVICES="" python3 repair.py

# Model daje z≈Çe wyniki
‚ùå Repair failed repeatedly
‚úÖ RozwiƒÖzanie: Spr√≥buj innego modelu z wiƒôkszymi zdolno≈õciami (--model deepseek-r1)
```

## üìà **Por√≥wnanie Wydajno≈õci v1.0 vs v2.0**

### **Analiza tego samego problemu (import_error):**

| Metryka | v1.0 (Static) | v2.0 (Dynamic) | Poprawa |
|---------|---------------|----------------|---------|
| **Zdolno≈õƒá modelu** | 85.00% (static) | 95.00% (dynamic) | **+10%** ‚Üë |
| **Prawdopodobie≈Ñstwo sukcesu** | 64.04% | 65.63% | **+1.59%** ‚Üë |
| **Koszt naprawy** | 1176.47 | 1052.63 | **-123.84** ‚Üì |
| **Dostƒôpne modele** | 1 (qwen) | 6 modeli | **+500%** ‚Üë |
| **Konfigurowalno≈õƒá** | Brak | YAML config | **Nowa funkcja** |
| **Uczenie siƒô** | Brak | Historical tracking | **Nowa funkcja** |
| **Auto-pobieranie** | Rƒôczne | Automatyczne | **Nowa funkcja** |

### **Kluczowe Ulepszenia:**

#### üéØ **Lepsza Analiza Decyzyjna**
- **Dynamiczne zdolno≈õci**: System uwzglƒôdnia rzeczywiste parametry modelu
- **Kategoryzacja problem√≥w**: 8 typ√≥w b≈Çƒôd√≥w vs generyczne podej≈õcie  
- **Historyczne uczenie**: System poprawia siƒô z ka≈ºdƒÖ naprawƒÖ
- **Ni≈ºsze koszty**: Lepsze zdolno≈õci = mniejszy koszt naprawy

#### ü§ñ **Szerszy Wyb√≥r Modeli**
```
v1.0: Tylko qwen2.5-coder (85% static capability)
v2.0: 6 modeli z dynamicznymi zdolno≈õciami:
‚îú‚îÄ qwen2.5-coder:7b    ‚Üí 95% (najlepszy do JSON/debugowania)
‚îú‚îÄ deepseek-r1:7b      ‚Üí 88% (reasoning capabilities)  
‚îú‚îÄ deepseek-coder:6.7b ‚Üí 80% (kod specjalizowany)
‚îú‚îÄ codellama:13b       ‚Üí 75% (du≈ºy kontekst 32k)
‚îú‚îÄ granite-code:8b     ‚Üí 70% (enterprise grade)
‚îî‚îÄ mistral:7b          ‚Üí 60% (fallback uniwersalny)
```

#### ‚öôÔ∏è **≈Åatwiejsza Konfiguracja**
```
v1.0: Twarde kodowanie parametr√≥w w kodzie
v2.0: Centralna konfiguracja YAML:
‚îú‚îÄ Optymalne ustawienia per model
‚îú‚îÄ Konfigurowalne timeouty  
‚îú‚îÄ Adaptacyjne parametry uczenia
‚îî‚îÄ ≈Åatwa customizacja bez edycji kodu
```

## ‚öôÔ∏è **Szczeg√≥≈Çowa Konfiguracja `llm.config.yaml`**

### **Pe≈Çna Struktura Pliku:**
```yaml
# Globalne ustawienia systemu
global:
  timeout: 60
  max_iterations: 5
  adaptive_evaluation:
    enabled: true
    history_weight: 0.3
    decay_factor: 0.9
    min_samples: 5
  capability_calculation:
    token_bonus_multiplier: 0.0001    # +0.01% za token ponad 8192
    temperature_penalty: 0.2          # -20% * temperatura
    context_bonus_multiplier: 0.0001  # +0.01% za token kontekstu ponad 8192
    max_capability: 0.95              # Maksymalna zdolno≈õƒá (95%)

# Konfiguracje poszczeg√≥lnych modeli
models:
  # Model domy≈õlny - najlepszy do napraw JSON
  qwen2.5-coder:
    model_name: "qwen2.5-coder:7b"
    max_tokens: 16384           # 2x wiƒôcej ni≈º standard
    temperature: 0.2            # Optymalna dla napraw
    retry_attempts: 3
    base_capability: 0.85       # Baza dla dynamicznej kalkulacji
    context_window: 32768       # Du≈ºy kontekst
    specialization: ["json", "debugging", "python", "javascript"]
    
  # Nowy model z reasoning - do z≈Ço≈ºonych problem√≥w
  deepseek-r1:
    model_name: "deepseek-r1:7b"
    max_tokens: 12288
    temperature: 0.1            # Niska dla reasoning
    retry_attempts: 4
    base_capability: 0.80
    context_window: 16384
    specialization: ["reasoning", "complex_logic", "algorithms"]
    
  # Du≈ºy model - do wiƒôkszych projekt√≥w
  codellama:
    model_name: "codellama:13b"
    max_tokens: 8192
    temperature: 0.3
    retry_attempts: 2
    base_capability: 0.70
    context_window: 32768       # Najwiƒôkszy kontekst
    specialization: ["large_codebases", "refactoring", "architecture"]
```

### **Customizacja dla W≈Çasnych Potrzeb:**

#### **Zwiƒôksz Zdolno≈õci Modelu:**
```yaml
models:
  custom-qwen:
    base_capability: 0.90      # ‚Üë Wy≈ºsza baza
    max_tokens: 32768          # ‚Üë Wiƒôcej token√≥w = bonus
    temperature: 0.1           # ‚Üì Ni≈ºsza temperatura = mniej penalty
    context_window: 65536      # ‚Üë Wiƒôkszy kontekst = bonus
```

#### **Dostosuj dla ≈örodowiska Produkcyjnego:**
```yaml
global:
  timeout: 120                 # ‚Üë Wiƒôcej czasu dla z≈Ço≈ºonych napraw
  max_iterations: 10           # ‚Üë Wiƒôcej pr√≥b
  adaptive_evaluation:
    history_weight: 0.5        # ‚Üë Wiƒôksza waga dla historii
    min_samples: 10            # ‚Üë Wiƒôcej danych do oceny
```

#### **Optymalizacja dla Szybko≈õci:**
```yaml
models:
  fast-mistral:
    max_tokens: 4096           # ‚Üì Mniej token√≥w = szybciej
    temperature: 0.4           # ‚Üë Wy≈ºsza = mniej precyzyjne ale szybsze
    retry_attempts: 1          # ‚Üì Mniej pr√≥b
```

### 7. **Raporty i Decyzje**

System generuje szczeg√≥≈Çowe raporty zawierajƒÖce:
- Analizƒô koszt√≥w (repair vs rebuild)
- Prawdopodobie≈Ñstwo sukcesu
- Zastosowane poprawki
- Ocenƒô ryzyka regresji
- Rekomendacje dalszych krok√≥w

### 8. **Wsparcie dla wielu jƒôzyk√≥w**

Automatycznie rozpoznaje i obs≈Çuguje:
- Python (FastAPI, Django, Flask)
- JavaScript/Node.js (Express, Next.js)
- Go (Gin, Fiber)
- Rust, Java, Ruby, PHP

Skrypt jest w pe≈Çni zintegrowany z podej≈õciem YMLL i implementuje wszystkie najlepsze praktyki z REPAIR_GUIDELINES, zapewniajƒÖc efektywny i powtarzalny proces naprawiania kodu z pomocƒÖ LLM.

## Funkcja logitowa

Funkcja **logitowa** to po prostu funkcja matematyczna u≈ºywana g≈Ç√≥wnie w statystyce i uczeniu maszynowym do przekszta≈Çcania prawdopodobie≈Ñstw w tzw. log-odds. Jest odwrotno≈õciƒÖ funkcji sigmoidalnej (logistycznej).

Dok≈Çadniej:

### Definicja

Je≈ºeli $p$ to prawdopodobie≈Ñstwo zdarzenia (0 < p < 1), funkcja logitowa jest zdefiniowana jako:

$$
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right)
$$

* $p/(1-p)$ to **odds** (szansa, ≈ºe zdarzenie nastƒÖpi vs ≈ºe nie nastƒÖpi)
* $\ln$ to logarytm naturalny

### Przyk≈Çad

* Je≈õli $p = 0.8$ (80% prawdopodobie≈Ñstwa),

$$
\text{logit}(0.8) = \ln\left(\frac{0.8}{0.2}\right) = \ln(4) \approx 1.386
$$

* Je≈õli $p = 0.5$, $\text{logit}(0.5) = \ln(1) = 0$

### Zastosowanie

* W **regresji logistycznej** logit przekszta≈Çca prawdopodobie≈Ñstwa w warto≈õƒá na osi liczbowej od $-\infty$ do $+\infty$, co pozwala modelowi liniowemu prognozowaƒá log-odds, a nastƒôpnie ≈Çatwo przekszta≈Çcaƒá z powrotem w prawdopodobie≈Ñstwo.
* W twoim kontek≈õcie (system naprawy kodu) funkcja logitowa mo≈ºe s≈Çu≈ºyƒá do obliczenia **prawdopodobie≈Ñstwa sukcesu naprawy** na podstawie r√≥≈ºnych zmiennych (jak z≈Ço≈ºono≈õƒá, dostƒôpno≈õƒá test√≥w itd.).

